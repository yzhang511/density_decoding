{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2eeb956a-e7c2-417f-b937-f996edf94acb",
   "metadata": {},
   "source": [
    "svi - decoder for continuous y:\n",
    "    \n",
    "the key is to represent important quantities as random variables, and then sample from them:\n",
    "    \n",
    "$s \\sim \\mathcal{N}(\\cdot, \\cdot)$,\n",
    "\n",
    "$z \\sim \\text{Categorical}(\\pi)$,\n",
    "\n",
    "$\\pi = \\lambda / \\sum \\lambda$,\n",
    "\n",
    "$\\log(\\lambda) \\sim \\mathcal{N}(b + \\beta y, 0)$,\n",
    "\n",
    "$y \\sim \\mathcal{GP}(\\cdot, \\cdot)$\n",
    "\n",
    "(remember to specify the priors for these R.V.s)\n",
    "\n",
    "when calculate the elbo, we can sample from these R.V.s and then use `pytorch`'s existing functionality to compute the log-likelihood and differential entropy terms.\n",
    "\n",
    "**caveat**: since a finite subset of GP is just a multivariate normal, we can set its mean and kernel function as parameters to be learned by SGD, and then do everything else w.r.t. the multivariate normal. (see [gaussian-process-tutorial](https://peterroelants.github.io/posts/gaussian-process-tutorial/))\n",
    "\n",
    "we can still keep the cavi encoder but implement a svi decoder. the principle is that if we can get exact estimates (cavi) then exact is prefered over stochastic (svi)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "169b9825-6096-4589-90bb-40d7b5e2c1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import torch\n",
    "import torch.distributions as D\n",
    "\n",
    "from clusterless import preprocess\n",
    "from clusterless import decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8946fa03-525d-4388-ad43-b0740fc38180",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 666\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.set_default_dtype(torch.double)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6184b37c-993a-4976-ad99-eeb205b80233",
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_SIZE = 15\n",
    "MEDIUM_SIZE = 20\n",
    "BIGGER_SIZE = 25\n",
    "\n",
    "plt.rc('font', size=MEDIUM_SIZE)         \n",
    "plt.rc('axes', titlesize=MEDIUM_SIZE)     \n",
    "plt.rc('axes', labelsize=MEDIUM_SIZE) \n",
    "plt.rc('axes', linewidth = 1.5)\n",
    "plt.rc('xtick', labelsize=MEDIUM_SIZE)   \n",
    "plt.rc('ytick', labelsize=MEDIUM_SIZE)   \n",
    "plt.rc('legend', fontsize=MEDIUM_SIZE)   \n",
    "plt.rc('figure', titlesize=MEDIUM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d961e4-8d82-48d4-90f6-cfcc98437449",
   "metadata": {},
   "source": [
    "#### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "40112036-8b3f-432b-a902-2c6323f0fe0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pid = 'dab512bd-a02d-4c1f-8dbc-9155a163efc0'\n",
    "rootpath = '/mnt/3TB/yizi/Downloads/ONE/openalyx.internationalbrainlab.org'\n",
    "trial_data_path = rootpath + '/danlab/Subjects/DY_016/2020-09-12/001/alf'\n",
    "neural_data_path = '/mnt/3TB/yizi/danlab/Subjects/DY_016'\n",
    "behavior_data_path = rootpath + '/paper_repro_ephys_data/figure9_10/original_data'\n",
    "save_path = '../saved_results/danlab/Subjects/DY_016/cavi_results/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b11221e0-0d6a-44f7-a187-f90eddfe46fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pid: dab512bd-a02d-4c1f-8dbc-9155a163efc0\n",
      "eid: d23a44ef-1402-4ed7-97f5-47e9a7a504d9\n",
      "1st trial stim on time: 17.56, last trial stim on time 2310.24\n"
     ]
    }
   ],
   "source": [
    "unsorted_trials, stim_on_times, np1_channel_map = preprocess.load_neural_data(\n",
    "    pid=pid, \n",
    "    trial_data_path=trial_data_path,\n",
    "    neural_data_path=neural_data_path,\n",
    "    behavior_data_path=behavior_data_path,\n",
    "    keep_active_trials=True, \n",
    "    roi='all',\n",
    "    kilosort=False,\n",
    "    triage=False\n",
    ")\n",
    "\n",
    "behave_dict = preprocess.load_behaviors_data(behavior_data_path, pid)\n",
    "_, wheel_velocity, _, _, _, _ = preprocess.preprocess_dynamic_behaviors(behave_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "77834d82-67e7-4a30-a891-a2a24fd55419",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader():\n",
    "    def __init__(self, data, y, stim_on_times, np1_channel_map, n_t_bins=30):\n",
    "        self.data = data\n",
    "        self.y = y\n",
    "        self.stim_on_times = stim_on_times\n",
    "        self.np1_channel_map = np1_channel_map\n",
    "        self.n_t_bins = n_t_bins\n",
    "        self.n_trials = stim_on_times.shape[0]\n",
    "        self.n_channels = np1_channel_map.shape[0]\n",
    "        self.t_binning = np.arange(0, 1.5, step = (1.5 - 0) / n_t_bins)\n",
    "        self.rand_trial_ids = np.arange(self.n_trials)\n",
    "        \n",
    "        # allocate unsorted data into trials\n",
    "        self.trial_ids = []\n",
    "        self.t_ids = []\n",
    "        self.trials = []\n",
    "        self.t_bins = []\n",
    "        for k in range(self.n_trials):\n",
    "            mask = np.logical_and(data[:,0] >= stim_on_times[k] - 0.5,\n",
    "                                  data[:,0] <= stim_on_times[k] + 1)\n",
    "            trial = data[mask,:]\n",
    "            trial[:,0] = trial[:,0] - trial[:,0].min()\n",
    "            t_bins = np.digitize(trial[:,0], self.t_binning, right = False) - 1\n",
    "            t_bin_lst = []\n",
    "            for t in range(self.n_t_bins):\n",
    "                t_bin = trial[t_bins == t,1:]\n",
    "                self.trial_ids.append(np.ones_like(t_bin[:,0]) * k)\n",
    "                self.t_ids.append(np.ones_like(t_bin[:,0]) * t)\n",
    "                t_bin_lst.append(t_bin)\n",
    "            self.trials.append(t_bin_lst)\n",
    "    \n",
    "    \n",
    "    def split_train_test(self, split=.8):\n",
    "        \n",
    "        self.train_ids = self.rand_trial_ids[:int(split * self.n_trials)]\n",
    "        self.test_ids = self.rand_trial_ids[int(split * self.n_trials):]\n",
    "        self.y_train = self.y[self.train_ids]\n",
    "        self.y_test = self.y[self.test_ids]\n",
    "        \n",
    "        trial_ids = np.concatenate(self.trial_ids)\n",
    "        t_ids = np.concatenate(self.t_ids)\n",
    "        trials = np.concatenate(np.concatenate(self.trials))\n",
    "\n",
    "        train_mask = np.sum([trial_ids==idx for idx in self.train_ids], axis=0).astype(bool)\n",
    "        test_mask = np.sum([trial_ids==idx for idx in self.test_ids], axis=0).astype(bool)\n",
    "        train_k_ids, test_k_ids = trial_ids[train_mask], trial_ids[test_mask]\n",
    "        train_t_ids, test_t_ids = t_ids[train_mask], t_ids[test_mask]\n",
    "        train_k_ids = [torch.argwhere(torch.tensor(train_k_ids)==k).reshape(-1) for k in self.train_ids]\n",
    "        train_t_ids = [torch.argwhere(torch.tensor(train_t_ids)==t).reshape(-1) for t in range(self.n_t_bins)]\n",
    "        test_k_ids = [torch.argwhere(torch.tensor(test_k_ids)==k).reshape(-1) for k in self.test_ids]\n",
    "        test_t_ids = [torch.argwhere(torch.tensor(test_t_ids)==t).reshape(-1) for t in range(self.n_t_bins)]\n",
    "        train_trials, test_trials = trials[train_mask], trials[test_mask]\n",
    "        \n",
    "        return train_trials, train_k_ids, train_t_ids, \\\n",
    "               test_trials, test_k_ids, test_t_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4077bb0d-08ab-4860-86d1-99cf6869de75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_loader = DataLoader(data = np.concatenate(unsorted_trials)[:,[0,2,3,4]], \n",
    "                         y = wheel_velocity, \n",
    "                         stim_on_times = stim_on_times, \n",
    "                         np1_channel_map = np1_channel_map, \n",
    "                         n_t_bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e01ea19c-2a77-4d2b-afdc-60902445a13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:180: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "train_trials, train_k_ids, train_t_ids, _, _, _ = data_loader.split_train_test(split=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "302554bd-8ebc-41a7-841a-64e092b21527",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GaussianMixture(init_params=&#x27;k-means++&#x27;, n_components=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GaussianMixture</label><div class=\"sk-toggleable__content\"><pre>GaussianMixture(init_params=&#x27;k-means++&#x27;, n_components=10)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GaussianMixture(init_params='k-means++', n_components=10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm = GaussianMixture(n_components=10, \n",
    "                      covariance_type='full', \n",
    "                      init_params='k-means++',\n",
    "                      verbose=0)\n",
    "gmm.fit(train_trials)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03422002-8fd1-44f8-8921-9d603f70d765",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.tensor(train_trials)\n",
    "y = torch.tensor(data_loader.y_train)\n",
    "\n",
    "Nk = len(data_loader.train_ids)\n",
    "Nt = data_loader.n_t_bins\n",
    "Nc = gmm.means_.shape[0]\n",
    "Nd = gmm.means_.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32774550-0ea7-4971-bc99-318d162deddf",
   "metadata": {},
   "source": [
    "#### decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6be5b3ff-d29d-4c92-858c-a1954fc4841b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_log(x, minval=1e-10):\n",
    "    return torch.log(x + minval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af678557-5d1b-4d18-a74a-72bd9f97f067",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(torch.nn.Module):\n",
    "\n",
    "    def __init__(self, \n",
    "                 Nk, Nt, Nc, Nd, \n",
    "                 init_means, init_covs, \n",
    "                 init_bs, init_betas,\n",
    "                 ks, ts):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.Nk = Nk\n",
    "        self.Nt = Nt\n",
    "        self.Nc = Nc\n",
    "        self.Nd = Nd\n",
    "        self.ks = ks\n",
    "        self.ts = ts\n",
    "        \n",
    "        # initialize variables for variational distribution\n",
    "        self.means = torch.nn.Parameter(init_means, requires_grad=True)\n",
    "        self.covs = torch.nn.Parameter(init_covs, requires_grad=True)\n",
    "        self.bs = torch.nn.Parameter(init_bs, requires_grad=False)\n",
    "        self.betas = torch.nn.Parameter(init_betas, requires_grad=False)\n",
    "        \n",
    "        # initialize mean and cov functions (kernels) for GP\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def forward(self, s, M=1000):\n",
    "        # M = number of Monte Carlo samples to be drawn\n",
    "        \n",
    "        # specify the variational dist. for y\n",
    "        # TO DO: use torch's multivariate_normal with learned mean and cov functions\n",
    "\n",
    "        \n",
    "        \n",
    "        # compute log-lambdas\n",
    "        log_lambdas = torch.zeros((self.Nk, self.Nc, self.Nt))\n",
    "        for k in range(self.Nk):\n",
    "            for t in range(self.Nt):\n",
    "                log_lambdas[k,:,t] = self.bs + self.betas[:,t] @ y[k]\n",
    "        \n",
    "        # compute mixing proportions \n",
    "        log_pis = log_lambdas - torch.logsumexp(log_lambdas, 1)[:,None,:]\n",
    "        \n",
    "        # specify the variational dist. for z\n",
    "        z = torch.zeros((Nk, Nt, Nc))\n",
    "        for k in range(Nk):\n",
    "            for t in range(Nt):\n",
    "                z[k,:,t] = D.categorical.Categorical(probs=torch.exp(log_pis)[k,:,t])\n",
    "        \n",
    "        # compute log-likelihood for s as mixture density\n",
    "        ll = torch.zeros((s.shape[0], self.Nc))\n",
    "        for j in range(self.Nc):\n",
    "            ll[:,j] = D.multivariate_normal.MultivariateNormal(\n",
    "                            loc=self.means[j], \n",
    "                            covariance_matrix=self.covs[j]\n",
    "                        ).log_prob(s)\n",
    "        \n",
    "        # Unlike CAVI where we have exact formula for q(z), \n",
    "        # we'd have to sample z from q in SVI.\n",
    "        # sampling z\n",
    "        q_z = torch.zeros((Nk, Nc, Nt))\n",
    "        for k in range(Nk):\n",
    "            for t in range(Nt):\n",
    "                z_sample = z[k,:,t].sample((M,))\n",
    "                for j in range(Nc):\n",
    "                    q_z[k,j,t] = torch.sum(z_sample == j)\n",
    "                \n",
    "        # TO DO: sampling y \n",
    "        \n",
    "        \n",
    "            \n",
    "        # compute ELBO = E_z,y[logp(s,z,y)] - E_z,y[logq(z,y)]\n",
    "        # E_z,y[logp(s,z,y)] = E_z,y[logp(s|z) + logp(z|y) + logp(y)]\n",
    "        \n",
    "        # E_z[logp(s|z)]\n",
    "        log_p_s_cond_z = 0\n",
    "        for k in range(Nk):\n",
    "            for t in range(Nt):\n",
    "                k_t_idx = np.intersect1d(self.ks[k], self.ts[t])\n",
    "                log_p_s_cond_z += q_z[k,:,t] * ll[k_t_idx]\n",
    "                \n",
    "        \n",
    "        # E_z[logp(z|y)]\n",
    "        log_p_z_cond_y = 0\n",
    "        for k in range(Nk):\n",
    "            for t in range(Nt):\n",
    "                log_p_z_cond_y += q_z[k,:,t] * log_pis[k,:,t]\n",
    "        \n",
    "        \n",
    "        # TO DO: E_y[logp(y)]\n",
    "        log_p_y = 0\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Entropy E_z[logq(z)]\n",
    "        entropy_z = (z * safe_log(z)).mean(-1).sum()\n",
    "        \n",
    "        # TO DO: Entropy E_y[logq(y)]\n",
    "        entropy_y = 0\n",
    "        \n",
    "        \n",
    "                \n",
    "        return log_p_s_cond_z + log_p_z_cond_y + log_p_y - entropy_z - entropy_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb113b6f-c39f-4113-a74e-49d2393a8d14",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e134817-9635-44ed-a098-a2d509a416af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a22e195-e3b2-4d61-a5ad-0b659a472732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
